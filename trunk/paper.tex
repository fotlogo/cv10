\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{subfigure}
\usepackage{algorithmic}
\usepackage{algorithm}

\newenvironment{tightenumerate}{
\begin{enumerate}
  \setlength{\itemsep}{1pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
}{\end{enumerate}
}

% this makes list spacing much better.
\newenvironment{tightitemize}{
\begin{itemize}
  \setlength{\itemsep}{1pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
}{\end{itemize}
}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifcvprfinal\pagestyle{empty}\fi

\begin{document}

\title{Segmentation Using Attributes}

\author{Aibo Tian\\
%Institution1\\
%Institution1 address\\
{\tt\small atian@cs.utexas.edu}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
John Edwards\\
%Institution2\\
%First line of institution2 address\\
{\tt\small edwardsj@cs.utexas.edu}
}

\maketitle
\thispagestyle{empty}

%-------------------------------------------------------------------------------
% abstract
%-------------------------------------------------------------------------------
\begin{abstract}
Attributes have been shown to be effective in assisting object detection and
classification, and they provide means to also incorporate semantic
information available to and provided by humans.  On another front, the
combination of segmentation and object classification has been shown to
improve both segmentation and classification.  We present a method of incorporating
attributes into segmentation using a hierarchical methodology.
That is, using attributes as features, we discover attributes of regions
at all levels of a hierarchical segmentation,
incorporate hierarchical confidences in determining a confidence that
a given region is part of a given object, and segment the image using
these confidences.  We also present encouraging segmentation results
from experiments run using this method.
\end{abstract}

%-------------------------------------------------------------------------------
% introduction
%-------------------------------------------------------------------------------
\section{Introduction}
Very recent work on using attributes as features \cite{farhadi09, lampert09}
has shown that attributes are an effective way of communicating information
about an object in an image.  Attributes are, in essence, semantic high-level
features.  The attributes of an image are learned using low-level features,
and once learned, they can be used in object detection and also in describing
objects in an image that are unknown.

Image segmentation is a very different field of research and has a very different
goal - that of finding very accurate boundaries of objects in an image.

Our work focuses on leveraging attributes to produce accurate segmentations.
The idea is to find image regions with salient attribute distributions.  If
these regions can be combined together to for larger regions with attribute
distributions matching some object's canonical distribution then one can
declare the combined region an accurate segmentation for that object.

\begin{figure}
\centering
\subfigure[]{
\includegraphics[width=0.22\textwidth]{figures/truck_orig_med.eps}
\label{fig:trucka}
}
\subfigure[]{
\includegraphics[width=0.22\textwidth]{figures/truck_final_med.eps}
\label{fig:truckb}
}
\caption{Segmentation by maximizing attribute saliency.  \subref{fig:trucka} shows
the original image with any ``car'' attributes that were predicted.  $P$ is the probability
that the image contains a car.  \subref{fig:truckb} shows the segmented image after
running our segmentation tree pruning algorithm.  As extraneous regions are pruned
more car attributes are predicted and the probability that the remaining image
contains a car increases.}
\label{fig:truck}
\end{figure}

In practice, however, the testing of all combinations of regions to
find objects is intractable.  We use two approximatory algorithms
operating on a segmentation hierarchy to determine salient regions.
Both are subtractive algorithms, which first predict attributes on
the entire region and then proceed by observing the effects of
removing different regions during a top-down traversal of the tree.
The benefit of using these methods are that large regions that
clearly don't contribute to attribute saliency can be pruned from
the tree early in the traversal.  This approach avoids the
combinatoric complexity of a full search and, even more, prunes many
regions from the segmentation tree before they even are searched,
bringing the computational complexity, in practice, to roughly
$O(n)$ where $n$ is the number of leaves in the segmentation tree.
The flowchart of our algorithm is shown in Figure
\ref{fig:flowchart}.

While we focus on segmentation of known objects, we anticipate an application
that takes more advantage of attributes: that of segmenting \emph{unknown}
objects from an image that exhibit salient but unrecognized attribute
distributions.  This type of application brings out some of the true power of
attributes, that of assigning semantic meaning to unknown, segmented objects.

%This paper is organized as follows: we first discuss related work (\S \ref{sec:related_work})
%and then present the algorithm in detail (\S \ref{sec:technical}).  We then discuss results from
%a small-scale experiment on images from the pascal 2008 dataset (\S \ref{sec:results}), after which we draw
%conclusions and discuss future directions (\S \ref{sec:conclusions}).

%-------------------------------------------------------------------------------
% related work
%-------------------------------------------------------------------------------
\section{Related work}
\label{sec:related_work}
[Note: this section was lifted straight from our extended abstract
and has not yet been updated to reflect what we actually did.]

A significant amount of recent work has been done in the area of using
attributes as high-level features for image classification and object
description \cite{farhadi09, lampert09, kumar09}.
Another area of computer vision, that of segmentation,
has seen a resurgence of popularity as methods to incorporate top-down
information into segmentation have been shown to be highly effective
\cite{borenstein04, pantofaru, gu09, russell06, malisiewicz, leibe04, hoiem05, shotton06}.
We propose to leverage research done in both of these areas -- attributes
and top-down/bottom-up segmentation -- to produce a combined approach.

Farhadi et al
\cite{farhadi09} use attributes as high-level features for object classification. They
can also describe an image using its attributes alone.
Lampert et al \cite{lampert09}
use the attributes as a intermediate layer between the low-level
features and object classes. Based on this layer, they can predict
disjoint classes that are not seen in the training set.  Both of these
approaches are geared toward object detection and classification.  While
our method goes further, incorporating segmentation, we will be using
the ideas and algorithms in \cite{farhadi09} for assignments of
attributes to different segmented regions.

Object classification can give some top-down high-level information
for segmentation. On the other hand, segmentation can provide
image-based coherence information for classification. To deal with
the problem that the bottom-up single segmentation is not accurate,
more and more algorithms are based on multiple segmentations. Gu et
al \cite{gu09} uses the hierarchical segmentation algorithm to
generate multiple segments, and detect different parts of objects in
each segment. The final objects are voted for by the various parts.
We will initially be using the same segmentation algorithm used in
this paper, but without discarding the hierarchy.
Russell et al \cite{russell06} detect the whole object in each of
the multiple segment, by assuming that most objects get segmented
correctly at least once during the multiple segmentation. Our approach
will be far less aggressive in that we will use object classes and
features (attributes in our case) that are known \emph{a priori}.
Pantofaru et
al \cite{pantofaru} use three different segmentation algorithms to
generate the multiple segments. After detecting objects in each
segment, they merge the intersected segments as final results. In
our method, we use common knowledge of distribution of attributes to
merge segments.
Borenstein et al \cite{borenstein04} proposed a method of combining top-down
and bottom-up information to generate a high-quality segmentation.  Their
approach to the top-down is to use exemplar image fragments and match them
to the image and then refining the top-down segmentation using their
segmentation hierarchy.  Our approach is very similar to this in spirit.
While our initial estimate of segmentation is from the bottom-up (while
theirs is from the top-down) we will use the hierarchical information
from the segmentation to boost confidences of segmentation at the lowest
level, similar to what they do.

Another interesting work is that of Gould et al \cite{gould08}.  They use
superpixels for an initial segmentation, build a CRF that includes a
relative location feature in the energy function and minimizes the CRF
for a final segmentation.  The relative location feature is determined
using prior location probability maps.  Our approach is similar to this
as well, in that we will also incorporate relative location information,
although ours will use attributes' relative locations rather than object
classes.  We will also use a CRF for the final segmentation, but will
use a min-cut solution since our labels are binary.
Kumar et al \cite{kumar05} incorporate shape structure into the segmentation by
adding a latent shape parameter to the CRF.  This seems like it would be
a useful addition to our system but make the scope too large.

\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{figures/flowchart.eps}
\caption{Flowchart of the proposed algorithm.} \label{fig:flowchart}
\end{figure}


%-------------------------------------------------------------------------------
% technical approach
%-------------------------------------------------------------------------------
\section{Attribute Detection}
\label{sec:attribute}

Attributes are high-level features which can reflect semantic properties of objects. Given the attributes, we can do object detection based on the their distribution. We use the same attribute set as \cite{farhadi09}. This set contains three types of attributes, including shape, part and material. All attributes are listed in figure \ref{fig:atts}.

\begin{figure}
\begin{tabular}{llll}
2D Boxy & 3D Boxy & Round & Vert Cyl \\
Horiz Cyl & Occluded & Tail & Beak \\
Head & Ear & Snout & Nose \\
Mouth & Hair & Face & Eye \\
Torso & Hand & Arm & Leg \\
Foot/Shoe & Wing & Propeller & Jet engine \\
Window & Row Wind & Wheel & Door \\
Headlight & Taillight & Side mirror & Exhaust \\
Pedal & Handlebars & Engine & Sail \\
Mast & Text & Label & Furn. Leg \\
Furn. Back & Furn. Seat & Furn. Arm & Horn \\
Rein & Saddle & Leaf & Flower \\
Stem/Trunk & Pot & Screen & Skin \\
Metal & Plastic & Wood & Cloth \\
Furry & Glass & Feather & Wool \\
Clear & Shiny & Vegetation & Leather \\
\end{tabular}
\caption{List of attributes trained.}
\label{fig:atts}
\end{figure}

We use similar algorithm as \cite{farhadi09} to train the attribute models. Three different type of features are used, such as HOG, texture and color descriptors\cite{farhadi09}. Those features can capture different properties. We use kmeans to quantize all the features. The HOG descriptors are quantized into 1000 clusters, and the texture and color are quantized to 256 and 128 clusters, respectively. All the quantized features are concatenated to form the 1384 deminsional feature.

For training, we use SVM classifier with Gaussian kernel to train
model for each attribute.

%The precision of all the attributes on the test set is illustrated
%in figure?.


%Our approach begins with training attribute classifiers.  We trained using
%an SVM with linear kernel.  A set of training images is annotated with objects
%contained in the image, bounding boxes for the objects, and attributes exhibited
%by the image.  We use a subset of the features used in \cite{farhadi09}, namely
%HOG, texture and color descriptors.  The HOG descriptors are quantized into 1000
%kmeans centers, and the texture and color are quantized to 256 and 128 kmeans centers,
%respectively.  Our features differ from \cite{farhadi09} in various ways.  We don't
%use edge descriptors, nor do we generate ``sub-''histograms for 6 cells in the
%image.  The presence of cell histograms would complicate matters when extracting
%features from segmented regions.

\section{Hierarchy Segmentation}
\label{sec:segmentation}

We use the segmentation approach of \cite{arbelaez09}.  This
produces a hierarchical segmentation by first detecting contours
using the \emph{gPb} detector \cite{maire08}.  Once the contours are
detected, regions are discovered using a watershed algorithm after
which an Ultrametric Contour Map (UCM) is constructed, which defines
the region hierarchy.  The UCM is stored as a grayscale image with
contours around the segmented regions.  These contours have intensities
representative of the region's location in the hierarchy.  So to find
the first branch in the tree, threshold the UCM image with a value
$t$ where $t = max{I(x,y)}$ where $I(x,y)$ is the intensity at pixel
location $(x,y)$.  The thresholded image can then be flood-filled into
regions.  These regions correspond to the branches on either side of
the root node in the segmentation tree.  The next level of the tree is
found by thresholding the UCM at the next highest intensity and so on.
Figure \ref{fig:ucm} shows an example UCM image and figure \ref{fig:tree}
shows the hierarchy generated from that UCM.

After the construction of segmentation tree, we detect all
the attributes in each node, including the intermediate and
leaf nodes. We use the output of SVM as the confidence (not binary),
so each node can be represented by the confidence distribution of
all attributes.  These distributions are stored and later used in
the additive pruning algorithm.  Unfortunately the subtractive algorithm
requires runtime predictions, and thus cannot use this preprocessing
step.



\section{Object Detection and Segmentation}
\label{sec:detection}

Our objective is to detect objects and segment them out. In our
segmentation tree, the problem changes to how to select
those nodes belong to the desired object. We estimate the
probability that a node is belong to object based on the attribute
confidence distribution. For each object, we manually give the
desired attribute distribution by common knowledge. We use the
histogram intersection between the desired distribution and the
estimated confidence distribution in each node as the similarity
measure. The histogram intersection is calculated as follows:

\[ HI(H_{model},H_{pred}) = \frac{|H_{model} \cap H_{pred}|}{|H_{model}|}
\] ,

where $H_{model}$ and $H_{pred}$ represent the desired attribute
distribution and predicted distribution.

To select the nodes, we propose two different top-down prune algorithms, additive algorithm and subtractive algorithm. We use the UCM to generate two types of region tree, shown in figure \ref{fig:tree}.
The first tree is an additive tree, where regions are subdivided into successively
smaller regions moving down in the tree.  We call it ``additive'' because the
original image can be reconstructed by adding the regions from each leaf node
together.  The figures show the nodes of the trees as containing images, but in
truth the trees contain binary masks which, when applied to the original image,
produce the shown node contents.

The second tree is ``subtractive.''  The tree has identical structure to the additive
tree and each node contains the inverse of the mask of the corresponding node
in the additive tree.

The final segmentation is obtained by pruning the region tree until a final segmentation
meeting some criterion of absolute or relative attribute saliency is found.
We have two methods for pruning the tree, each showing relative strengths and
weakness.  The first is a semi-greedy additive approach which uses the additive
tree.  The second is a greedy subtractive approach.  We now discuss each of these
in detail.

\begin{figure}
\centering
\includegraphics[width=0.30\textwidth]{figures/2008_000052_ucm.bmp.eps}
\caption{A UCM image from which a segmentation tree is derived.}
\label{fig:ucm}
\end{figure}

\begin{figure}
\centering
%\subfigure[]{
%\label{fig:treea}
\includegraphics[width=0.45\textwidth]{figures/tree_additive.eps}
%}
%\subfigure[]{
%\label{fig:treeb}
%\includegraphics[width=0.45\textwidth]{figures/tree_subtractive.eps}
%}
\caption{The segmentation hierarchy used by the additive
and subtractive algorithms.  This tree was generated from the UCM
in figure \ref{fig:ucm}.}
\label{fig:tree}
\end{figure}

\subsection{Additive algorithm}
\label{ssec:additive}

This pruning algorithm is based on three observations. (1) At least
one node in the segmentation tree contrains the whole object. (This
is always true since the root node contains the whole image). (2) If
both parent node and child node have high confidence of containing
the object, the segmentation of the child node is more accurate than
that of the parent node. (3) Even if some node has high confidence
of containing the object, its segmentation can still be improved by
removing irrelevant children node. The details of the additive
algorithm is shown in algorithm \ref{alg:add_prune}.

The idea of the algorithm is to find the smallest discriminative
regions, either positive or negative. We combine all the smallest
positive regions while remove all the negative ones to form the
final segmentation. We use three threshold for the similarity
measure, $thre\_perf$, $thre\_pos$ and $thre\_neg$ which represent
perfect segment, positive segment and negative segment,
respectively. If the similarity is above $thre\_perf$, which means
the segmentation is perfect for the object, we cut all the nodes
below and return this segment. If the similarity is below
$thre\_neg$, which means the segment does not contain the object, we
cut all the nodes below and return NULL. If the similarity is
between $thre\_perf$ and $thre\_pos$, which means the node
definitely contains the object but the segmentation is not perfect
yet, we keep on checking its children nodes to refine the
segmentation. The final case is that the similarity is between
$thre\_pos$ and $thre\_neg$, which means we are not sure whether
there is an object in the node, so we just keep on searching its
children.

The object detection is based on the output of ADDITIVE\_PRUNE. If the output is not NULL, which means at least one node in the tree has similarity above $thre\_pos$, we think the object is detected in this image.

(We also plan to do the vertical confidence propogation. But due to the slowness of the experiment, we cannot incorporate it in this draft. We will show it in the final paper.)


\begin{algorithm}
\begin{algorithmic}
  \STATE \textbf{input:}
  \STATE \hspace{3 mm} node $n$ in segmentation tree
  \STATE \hspace{3 mm} desired object model $obj\_model$
  \STATE \hspace{3 mm} threshold for perfect segment $thre\_perf$
  \STATE \hspace{3 mm} threshold for positive segment $thre\_pos$
  \STATE \hspace{3 mm} threshold for negative segment $thre\_neg$
  \STATE \textbf{output:}
  \STATE \hspace{3 mm} refined segment
  \STATE
  \STATE $sim=HI(obj\_model,n.conf)$
  \IF{$sim \ge thre\_perf$}
    \STATE return $n.segment$
  \ENDIF
  \IF{$sim \le thre\_neg$}
    \STATE return $NULL$
  \ENDIF
  \STATE $s'=NULL$
  \FOR{$i=1$ to $n.childNum$}
    \STATE s\{i\}=ADDITIVE\_PRUNE(n.children\{i\},obj\_model,...
    \STATE ... thre\_perf,thre\_pos,thre\_neg)
    \STATE s'=s'+s\{i\}
  \ENDFOR
  \IF{$sim\ge thre\_pos$ and $s'==NULL$}
    \STATE return n.segment
  \ELSE
    \STATE return s'
  \ENDIF
\end{algorithmic}
\caption{ADDITIVE\_PRUNE}
\label{alg:add_prune}
\end{algorithm}



\subsection{Subtractive algorithm}

The subtractive algorithm relies on the idea that if removing a region from
an image doesn't significantly affect the probability of the image containing
the object (which probability is obtained using the predicted attribute
distribution and is here used interchangably used with the term ``similarity'')
then that region can be safely removed in the final segmentation.
The algorithm itself is shown in algorithm \ref{alg:sub_prune}.

\begin{algorithm}
\begin{algorithmic}
  \STATE \textbf{input:}
  \STATE \hspace{3 mm} node $n$ in segmentation tree
  \STATE \hspace{3 mm} master mask $m_m$
  \STATE \hspace{3 mm} similarity $sim_{root}$ on root node
  \STATE \hspace{3 mm} factor $f$
  \STATE \textbf{output:}
  \STATE \hspace{3 mm} mask $m_m$
  \STATE
  \STATE $m_n$ := mask of $n$
  \STATE $r = m_{m} \cap \verb+~+ m_{n}$
  \STATE $pred\_model$ := predict attributes on image($r$)
  \STATE $sim=HI(obj\_model,pred\_model)$
  %\STATE $H_{pred}$ := predict attributes on image($m_n \cap m_m$)
  %\STATE $\displaystyle P_{pred} = \frac{H_{pattern} \cap H_{pred}}{H_{pattern} \cup H_{pred}}$
  \IF{$sim < f sim_{root}$}
    \FOR{$i=1$ to $n.childNum$}
      \STATE s\{i\}=SUBTRACTIVE\_PRUNE(n.children\{i\},$m_m$,
      \STATE ...$sim_{root}$, $f$)
    \ENDFOR
    %\STATE SUBTRACTIVE\_PRUNE(n.children)
  \ELSE
    \STATE $m_m = m_m \cap \verb+~+ m_n$
  \ENDIF
\end{algorithmic}
\caption{SUBTRACTIVE\_PRUNE}
\label{alg:sub_prune}
\end{algorithm}

The algorithm begins at the root of the tree and predicts attributes.
This attribute distribution and its associated similarity $sim_{root}$
to the object's canonical distribution is taken to be
the standard for the image.  That is, any final segmentation should not have
significantly less similarity $sim$ than $sim_{root}$.  The algorithm traverses
the subtractive tree depth-first.  At each node, attributes are predicted for 
a region $r$.  
$r$ is obtained by taking the intersection of the master mask and the inverted
node mask ($r = m_{m} \cap \verb+~+ m_{n}$).  The intuition is this: the master mask is our current best guess at
the region that contains an object.  By taking the intersection, we effectively
subtract the node's region out.
We can then predict on the new region.  If 
there is no significant negative effect on the similarity as
compared to the standard, then we assume that we can safely prune off the node's
region without removing a part of the object..  If the
similarity was significantly affected, then we assume that the region
contains some part of the object and we allow it to remain.  But that 
doesn't mean that sub-regions might not be able to be pruned
out, so the procedure is called recursively, checking for smaller regions
that might be able to be pruned.

As mentioned, if the similarity score was not significantly affected by
predicting on the master mask with the region removed, then the region is
permanently removed from the master mask
and no recursive call is made.  This is important, because it means
that we can remove large chunks of the image wholesale and bypass predicting
on the node's posterity.
The runtime performance of the algorithm is dominated by the prediction step.
Thus the advantage of pruning large chunks early is highly beneficial to
the runtime of the algorithm.

$f$ is an important parameter that requires tuning.  Increasing $f$ makes the
algorithm more conservative, that is, less likely to apply removals.  Decreasing
$f$ makes it more liberal.  We found $f=0.9$ to be a reasonable value for most images.
Figure \ref{fig:f} shows results of adjusting $f$ between 0.9 and 1.0.

The subtractive algorithm knows nothing about the number of objects present
in the image.  It attempts to segment any object out into the final region.
This is evident in figure \ref{fig:sub1}.  The mobile homes in the background
have car attributes and thus are included in the final segmentation, even
though they take a far less prominent role in the unsegmented image.  Another
example is figure \ref{fig:truck}, where the two cars visually behind the truck
are segmented alongside the truck.

\begin{figure}
\centering
\subfigure[]{
\label{fig:fa}
\includegraphics[width=0.22\textwidth]{figures/parking_0.9.eps}
}
\subfigure[]{
\label{fig:fb}
\includegraphics[width=0.22\textwidth]{figures/parking_1.0.eps}
} \caption{Result of adjusting the factor $f$ in the subtractive
algorithm. \subref{fig:fa} shows running the algorithm with $f=0.9$
and \subref{fig:fb} shows running the algorithm with $f=1.0$.  Note
that \subref{fig:fb} retains more of the cars, especially the two
closest on the right and left, but also has more false-positive
pixels, such as the ceiling region of the parking garage.}
\label{fig:f}
\end{figure}

\begin{figure}
\centering
\subfigure[]{
\label{fig:sub1a}
\includegraphics[width=0.22\textwidth]{figures/results/2008_000052.jpg.eps}
}
\subfigure[]{
\label{fig:sub1b}
\includegraphics[width=0.22\textwidth]{figures/2008_000052_1.00.jpg.eps}
} \caption{Subtractive segmentation on an image with multiple objects.
An observer may not even notice the trailer homes in the background in
the original image \subref{fig:sub1a},
but the segmentation includes a portion of them in the final segmentation \subref{fig:sub1b}.
Subtractive segmentation attempts to segment all objects present
in the image.}
\label{fig:sub1}
\end{figure}

%-------------------------------------------------------------------------------
% experimental results
%-------------------------------------------------------------------------------
\section{Experimental results}
\label{sec:results}

In this section, we evaluate our attribute based object detection
and segmentation algorithm. We will answer four questions. (1) What
is the performance of object detection and segmentation? (2) What is
the influences of different parameters? (3) What are the advantages
of the two pruning algorithms? (4) Is vertical confidence
propagation useful? (In this draft, we only have partial results, so
not all the questions are considered.)

%\subsection{Dataset}
%\label{ssec:dataset}

We use the Pascal2008 dataset. This dataset has a lot of real life
images which are more difficult to classify. The objects are labeled
by bounding boxes not accurate segmentation. For training the
attributes, we use the pascal2008 training set. The attribute
annotations used in \cite{farhadi09} are freely available and we
used them for training too. For testing the classification and
segmentation, we use the pascal2008 validation set. We plan to do
three classes classification and segmentation, including car,
aeroplane and horse. Now we only got the results on car. We will
incorporate the other classes in the final version.

Table \ref{table:aresults} shows some of the results from running
the additive algorithm on all cars in the dataset.  The recall is
the rate of correctly detecting images with cars.  The segmentation
precision is calculated with
\[ V(M_{groundtruth},M_{pred}) = \frac{|M_{groundtruth} \cap M_{pred}|}{|M_{groundtruth} \cup M_{pred}|} \]
where $M_{groundtruth}$ is the groundtruth mask calculated from the
bounding boxes stored with the annotations and $M_{pred}$ is the
predicted segmentation mask.  We of course recognize that the
bounding boxes are not a good measure of segmentation truth, but
it's the best we have available.

The segmentation precision for the additive algorithm
is far better than the precision for the subtractive algorithm (which averaged
0.09).  This is because the additive algorithm only segments images for
which it has predicted presence of a car, so that any image pruned by the
algorithm already has a high car similarity score, enhancing the segmentation.
The subtractive algorithm, on the other hand, assumes presence of a car in
the image and proceeds no matter how poor the initial similarity score is.
(For the final paper we expect to have results comparing the precision
of the two algorithms based only on images segmented by both.)

Figures \ref{fig:good_results} and \ref{fig:bad_results} show the
results of segmentations of various images by both pruning
algorithms. Figure \ref{fig:good_results} shows some good results,
while Figure \ref{fig:bad_results} shows some bad results. This
shows clear linkage between prominence of the side profile of the
car and success of the segmentation. It appears that the attribute
classifiers are more accurate on side profiles than front and rear
shots, although rear shots performed reasonably well when tail
lights were not illuminated.

\begin{table}
\centering
\begin{tabular}{|l|l|l|}
\hline Methods & Precision(\%) & Recall(\%) \\
\hline LowFea  & 55.97 & 70.96 \\
\hline Att1    & 51.89 & 51.89 \\
\hline Random  & 35.19 & 50.00 \\
\hline Add     & 60.82 & 54.06 \\
\hline
\end{tabular}
\caption{Object classification results of three baseline methods and the proposed additive algorithm.}
\label{table:aresults}
\end{table}

\begin{figure*}[p]
\centering
\begin{tabular}{ p{4cm} p{4cm} p{4cm} p{4cm} }
\includegraphics[width=3.95cm]{figures/results/2008_000052.jpg.eps} &
\includegraphics[width=3.95cm]{figures/results/2008_000828.jpg.eps} &
\includegraphics[width=3.95cm]{figures/results/2008_000952.jpg.eps} &
\includegraphics[width=3.95cm]{figures/results/2008_002198.jpg.eps} \\
\includegraphics[width=3.95cm]{figures/results/a2008_000052.jpg.eps} &
\includegraphics[width=3.95cm]{figures/results/a2008_000828.jpg.eps} &
\includegraphics[width=3.95cm]{figures/results/a2008_000952.jpg.eps} &
\includegraphics[width=3.95cm]{figures/results/a2008_002198.jpg.eps} \\
\includegraphics[width=3.95cm]{figures/results/b2008_000052.jpg.eps} &
\includegraphics[width=3.95cm]{figures/results/b2008_000828.jpg.eps} &
\includegraphics[width=3.95cm]{figures/results/b2008_000952.jpg.eps} &
\includegraphics[width=3.95cm]{figures/results/b2008_002198.jpg.eps} \\
\end{tabular}
\caption{Selected results from segmentation of cars from the Pascal2008
dataset.  The first row shows the original image.  The second row shows
the best segmentation from the additive algorithm.  The third row shows
the segmentation from the subtractive algorithm with $f=0.9$.}
\label{fig:good_results}
\end{figure*}

\begin{figure*}[p]
\centering
\begin{tabular}{ p{4cm} p{4cm} p{4cm} p{4cm} }
\includegraphics[width=3.95cm]{figures/results/2008_002483.jpg.eps} &
\includegraphics[width=3.95cm]{figures/results/2008_007466.jpg.eps} &
\includegraphics[width=3.95cm]{figures/results/2008_005378.jpg.eps} &
\includegraphics[width=3.95cm]{figures/results/2008_007529.jpg.eps} \\
\includegraphics[width=3.95cm]{figures/results/a2008_002483.jpg.eps} &
\includegraphics[width=3.95cm]{figures/results/a2008_007466.jpg.eps} &
%\includegraphics[width=3.95cm]{figures/results/a2008_005378.jpg.eps} &
Car not detected &
\includegraphics[width=3.95cm]{figures/results/a2008_007529.jpg.eps} \\
\includegraphics[width=3.95cm]{figures/results/b2008_002483.jpg.eps} &
\includegraphics[width=3.95cm]{figures/results/b2008_007466.jpg.eps} &
\includegraphics[width=3.95cm]{figures/results/b2008_005378.jpg.eps} &
\includegraphics[width=3.95cm]{figures/results/b2008_007529.jpg.eps} \\
\end{tabular}
\caption{Selected results from segmentation of cars from the Pascal2008
dataset.  The first row shows the original image.  The second row shows
the best segmentation from the additive algorithm.  The third row shows
the segmentation from the subtractive algorithm with $f=0.9$.}
\label{fig:bad_results}
\end{figure*}


\begin{figure*}[p]
\centering
\begin{tabular}{ p{4cm} p{4cm} p{4cm} p{4cm} }
\includegraphics[width=3.95cm]{figures/add_res/car/2008_007064.jpg.eps} &
\includegraphics[width=3.95cm]{figures/add_res/car/2008_000052.jpg.eps} &
\includegraphics[width=3.95cm]{figures/add_res/car/2008_007171.jpg.eps} &
\includegraphics[width=3.95cm]{figures/add_res/car/2008_000251.jpg.eps} \\
\includegraphics[width=3.95cm]{figures/add_res/car/2008_007064.jpg_1_good.jpg.eps} &
\includegraphics[width=3.95cm]{figures/add_res/car/2008_000052.jpg_1_good.jpg.eps} &
\includegraphics[width=3.95cm]{figures/add_res/car/2008_007171.jpg_1_good.jpg.eps} &
\includegraphics[width=3.95cm]{figures/add_res/car/2008_000251.jpg_1_good.jpg.eps} \\
\end{tabular}
\caption{Selected results from segmentation of cars from the Pascal2008
dataset.  The first row shows the original image.  The second row shows
the best segmentation from the additive algorithm.  The third row shows
the segmentation from the subtractive algorithm with $f=0.9$.}
\label{fig:good_results_car}
\end{figure*}


\begin{figure*}[p]
\centering
\begin{tabular}{ p{4cm} p{4cm} p{4cm} p{4cm} }
\includegraphics[width=3.95cm]{figures/add_res/car/2008_000868.jpg.eps} &
\includegraphics[width=3.95cm]{figures/add_res/car/2008_000884.jpg.eps} &
\includegraphics[width=3.95cm]{figures/add_res/car/2008_001830.jpg.eps} &
\includegraphics[width=3.95cm]{figures/add_res/car/2008_001867.jpg.eps} \\
\includegraphics[width=3.95cm]{figures/add_res/car/2008_000868.jpg_1_bad.jpg.eps} &
\includegraphics[width=3.95cm]{figures/add_res/car/2008_000884.jpg_1_bad.jpg.eps} &
\includegraphics[width=3.95cm]{figures/add_res/car/2008_001830.jpg_1_bad.jpg.eps} &
\includegraphics[width=3.95cm]{figures/add_res/car/2008_001867.jpg_1_bad.jpg.eps} \\
\end{tabular}
\caption{Selected results from segmentation of cars from the Pascal2008
dataset.  The first row shows the original image.  The second row shows
the best segmentation from the additive algorithm.  The third row shows
the segmentation from the subtractive algorithm with $f=0.9$.}
\label{fig:good_results_car}
\end{figure*}



\begin{figure*}[p]
\centering
\begin{tabular}{ p{4cm} p{4cm} p{4cm} p{4cm} }
\includegraphics[width=3.95cm]{figures/add_res/plane/2008_000021.jpg.eps} &
\includegraphics[width=3.95cm]{figures/add_res/plane/2008_001971.jpg.eps} &
\includegraphics[width=3.95cm]{figures/add_res/plane/2008_002673.jpg.eps} &
\includegraphics[width=3.95cm]{figures/add_res/plane/2008_002138.jpg.eps} \\
\includegraphics[width=3.95cm]{figures/add_res/plane/2008_000021.jpg_2_good.jpg.eps} &
\includegraphics[width=3.95cm]{figures/add_res/plane/2008_001971.jpg_2_good.jpg.eps} &
\includegraphics[width=3.95cm]{figures/add_res/plane/2008_002673.jpg_2_good.jpg.eps} &
\includegraphics[width=3.95cm]{figures/add_res/plane/2008_002138.jpg_2_good.jpg.eps} \\
\end{tabular}
\caption{Selected results from segmentation of cars from the Pascal2008
dataset.  The first row shows the original image.  The second row shows
the best segmentation from the additive algorithm.  The third row shows
the segmentation from the subtractive algorithm with $f=0.9$.}
\label{fig:good_results_car}
\end{figure*}


\begin{figure*}[p]
\centering
\begin{tabular}{ p{4cm} p{4cm} p{4cm} p{4cm} }
\includegraphics[width=3.95cm]{figures/add_res/plane/2008_000804.jpg.eps} &
\includegraphics[width=3.95cm]{figures/add_res/plane/2008_001805.jpg.eps} &
\includegraphics[width=3.95cm]{figures/add_res/plane/2008_002358.jpg.eps} &
\includegraphics[width=3.95cm]{figures/add_res/plane/2008_003673.jpg.eps} \\
\includegraphics[width=3.95cm]{figures/add_res/plane/2008_000804.jpg_2_bad.jpg.eps} &
\includegraphics[width=3.95cm]{figures/add_res/plane/2008_001805.jpg_2_okay.jpg.eps} &
\includegraphics[width=3.95cm]{figures/add_res/plane/2008_002358.jpg_2_okay.jpg.eps} &
\includegraphics[width=3.95cm]{figures/add_res/plane/2008_003673.jpg_2_bad.jpg.eps} \\
\end{tabular}
\caption{Selected results from segmentation of cars from the Pascal2008
dataset.  The first row shows the original image.  The second row shows
the best segmentation from the additive algorithm.  The third row shows
the segmentation from the subtractive algorithm with $f=0.9$.}
\label{fig:good_results_car}
\end{figure*}


\begin{figure*}[p]
\centering
\begin{tabular}{ p{4cm} p{4cm} p{4cm} p{4cm} }
\includegraphics[width=3.95cm]{figures/add_res/horse/2008_001542.jpg.eps} &
\includegraphics[width=3.95cm]{figures/add_res/horse/2008_001992.jpg.eps} &
\includegraphics[width=3.95cm]{figures/add_res/horse/2008_002172.jpg.eps} &
\includegraphics[width=3.95cm]{figures/add_res/horse/2008_002509.jpg.eps} \\
\includegraphics[width=3.95cm]{figures/add_res/horse/2008_001542.jpg_3_good.jpg.eps} &
\includegraphics[width=3.95cm]{figures/add_res/horse/2008_001992.jpg_3_good.jpg.eps} &
\includegraphics[width=3.95cm]{figures/add_res/horse/2008_002172.jpg_3_good.jpg.eps} &
\includegraphics[width=3.95cm]{figures/add_res/horse/2008_002509.jpg_3_good.jpg.eps} \\
\end{tabular}
\caption{Selected results from segmentation of cars from the Pascal2008
dataset.  The first row shows the original image.  The second row shows
the best segmentation from the additive algorithm.  The third row shows
the segmentation from the subtractive algorithm with $f=0.9$.}
\label{fig:good_results_car}
\end{figure*}


\begin{figure*}[p]
\centering
\begin{tabular}{ p{4cm} p{4cm} p{4cm} p{4cm} }
\includegraphics[width=3.95cm]{figures/add_res/horse/2008_000403.jpg.eps} &
\includegraphics[width=3.95cm]{figures/add_res/horse/2008_002835.jpg.eps} &
\includegraphics[width=3.95cm]{figures/add_res/horse/2008_003055.jpg.eps} &
\includegraphics[width=3.95cm]{figures/add_res/horse/2008_004470.jpg.eps} \\
\includegraphics[width=3.95cm]{figures/add_res/horse/2008_000403.jpg_3_bad.jpg.eps} &
\includegraphics[width=3.95cm]{figures/add_res/horse/2008_002835.jpg_3_bad.jpg.eps} &
\includegraphics[width=3.95cm]{figures/add_res/horse/2008_003055.jpg_3_okay.jpg.eps} &
\includegraphics[width=3.95cm]{figures/add_res/horse/2008_004470.jpg_3_bad.jpg.eps} \\
\end{tabular}
\caption{Selected results from segmentation of cars from the Pascal2008
dataset.  The first row shows the original image.  The second row shows
the best segmentation from the additive algorithm.  The third row shows
the segmentation from the subtractive algorithm with $f=0.9$.}
\label{fig:good_results_car}
\end{figure*}


%-------------------------------------------------------------------------------
% conclusions and future work
%-------------------------------------------------------------------------------
\section{Discussion}
\label{sec:discussion}
It can be seen that in simple cases both additive and subtractive algorithms
are able to segment out rough outlines of the objects.  In most cases, even
the more difficult ones, at least part of the car was present in the final
segmentation.  This shows that the segmentation algorithms appear to be
reasonable approximations to a combinatoric approach -- that of predicting
on every combination of regions in the image.

Despite these encouraging initial results, the final segmentation region
was often not coherent.
This appears to be largely because the region tree approach is not
constrained by spatial coherence.  That is, two leaf nodes containing image
regions that are spatially next to each other may be very distant in the
region tree.  Because of this, the attribute distribution may change significantly
after considering the first node and before consideration of the second node.
Another reason for this may be that our canonical attribute distribution for
our objects gave equal weights to each attribute.  For example, a wheel was
equally as important as shininess.  This gave unfair disadvantages to
under-represented or ill-trained attributes.  This can be seen in the results
where the body of the car is removed while the wheels or headlights are
retained.

As object detection is generally only as good as the features used, so also it
appears that this approach to segmentation is only as good as the attribute
selection and training.  We found that in images with poor segmentations, the
confidences in prediction of car attributes were generally low.  One problem
with this implementation of attribute classifiers was that we didn't use
the selection criterion described in \cite{farhadi09} that enhances within
category prediction ability.  This means that, for example, the wheel attribute
is highly similar to the car attribute and thus its discrimative ability
is hurt.  One future direction for this research would be to improve the attribute
training using the additional selection criterion and observe effects on
the segmentations.

An interesting orthogonal direction that could be taken with these ideas is to
use segmentation using attributes as a diagnostic tool for attribute classifiers.
That is, the segmentations produced by our algorithms can help zero in on areas
in an image that exhibit high responses in detection of certain attributes.
For example, an image could be segmented using similarity to a canonical distribution
using only one attribute.  If that attribute is ``fur'' and segmentations
consistently show paws and tails, then it would quickly become clear that
the fur attribute classifier has learned on something other than fur.

The holy grail of this approach, of course, is to be able to segment objects
for which the canonical attribute distribution is unknown, based on a measure
of attribute coherence.  If this can be achieved, then segmentation experiments
could open up to non-experts, who could define coherence measures based on
semantic attributes and then could use this approach to automatically segment large
numbers of images even if no detection of their desired object exists.

Another problem with the attribute based method is that the attributes are not discriminative 
enough to separate different objects. For example, if we use the general attributes of head, torso and leg to 
do the object classification, we cannot separate dog and cat, since both of them have all of the attributes. 
But those attributes can definitely separate the two objects from non-animal objects, such as car. This example demonstrates that 
attributes are helpful for the object classification, but they are not discriminative enough. So one way to deal with this is to use 
both attributes features and low level objects features for object classification. We can train a general attributes models first. 
When we want to classify some specific objects, we can incorporate a small number of low level features from the specific object samples to the attributes 
features. In this way, we both reduce the training samples for object classification, and make the attribute based method more discriminative.



\subsection*{Acknowledgements}
Thanks to the authors of \cite{arbelaez09} and \cite{farhadi09} for use of their
segmentation and feature extraction code, respectively.

%-------------------------------------------------------------------------------
% bibliography
%-------------------------------------------------------------------------------
{\small
%\bibliographystyle{plain}
\bibliographystyle{ieee}
\bibliography{refs}
}

\end{document}
